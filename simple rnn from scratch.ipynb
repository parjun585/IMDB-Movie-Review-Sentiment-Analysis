{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "# import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.datasets import imdb\n",
    "# Load the IMDB dataset from tensorflow_datasets\n",
    "(ds_train, ds_test), ds_info = imdb.load_data('imdb_reviews', \n",
    "                                         split=['train', 'test'], \n",
    "                                         as_supervised=True, \n",
    "                                         with_info=True)\n",
    "\n",
    "# Preprocess the dataset\n",
    "BUFFER_SIZE = 10000\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE = 10000\n",
    "MAX_LEN = 500\n",
    "\n",
    "# Encode the text into sequences of integers\n",
    "encoder = tf.keras.layers.TextVectorization(max_tokens=VOCAB_SIZE, output_mode='int', output_sequence_length=MAX_LEN)\n",
    "encoder.adapt(ds_train.map(lambda text, label: text))\n",
    "\n",
    "# Function to preprocess the dataset\n",
    "def encode_map_fn(text, label):\n",
    "    encoded_text = encoder(text)\n",
    "    return encoded_text, label\n",
    "\n",
    "# Apply the encoding to the train and test datasets\n",
    "ds_train = ds_train.map(encode_map_fn).cache().shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "ds_test = ds_test.map(encode_map_fn).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "# Build the RNN model using the functional API\n",
    "input_layer = layers.Input(shape=(MAX_LEN,), dtype=tf.int32)\n",
    "\n",
    "# Add the embedding layer\n",
    "embedding_layer = layers.Embedding(input_dim=VOCAB_SIZE, output_dim=128, input_length=MAX_LEN)(input_layer)\n",
    "\n",
    "# Add the SimpleRNN layer\n",
    "rnn_layer = layers.SimpleRNN(128, activation='relu')(embedding_layer)\n",
    "\n",
    "# Add the output layer\n",
    "output_layer = layers.Dense(1, activation='sigmoid')(rnn_layer)\n",
    "\n",
    "# Create the model\n",
    "model = Model(inputs=input_layer, outputs=output_layer)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Display the model summary\n",
    "model.summary()\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(ds_train, epochs=10, validation_data=ds_test)\n",
    "\n",
    "# Evaluate the model on the test dataset\n",
    "test_loss, test_acc = model.evaluate(ds_test)\n",
    "print(f'Test Accuracy: {test_acc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.datasets import imdb\n",
    "\n",
    "# Load the IMDB dataset\n",
    "(train_data, train_labels), (test_data, test_labels) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Convert data to one-hot vectors\n",
    "train_data = tf.keras.utils.to_categorical(train_data, 10000)\n",
    "test_data = tf.keras.utils.to_categorical(test_data, 10000)\n",
    "\n",
    "# Create a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add an embedding layer to convert word indices to dense vectors\n",
    "model.add(Embedding(10000, 128))\n",
    "\n",
    "# Add a SimpleRNN layer\n",
    "model.add(SimpleRNN(128))\n",
    "\n",
    "# Add a Dense layer for classification\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(train_data, train_labels, epochs=10, batch_size=32, validation_data=(test_data, test_labels))\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(test_data, test_labels)\n",
    "print('Test accuracy:', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.layers import Input, Embedding, SimpleRNN, Dense\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load the IMDB dataset\n",
    "(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=10000)\n",
    "\n",
    "# Pad sequences to a fixed length\n",
    "max_length = 500\n",
    "x_train = tf.keras.preprocessing.sequence.pad_sequences(x_train, maxlen=max_length)\n",
    "x_test = tf.keras.preprocessing.sequence.pad_sequences(x_test, maxlen=max_length)\n",
    "\n",
    "# Define the model architecture using Functional API\n",
    "vocab_size = 10000  # Number of unique words to consider\n",
    "embedding_dim = 16  # Dimension of the embedding layer\n",
    "rnn_units = 32      # Number of RNN units\n",
    "\n",
    "# Functional API: Define the input and the model structure\n",
    "inputs = Input(shape=(max_length,))\n",
    "x = Embedding(vocab_size, embedding_dim)(inputs)\n",
    "x = SimpleRNN(rnn_units)(x)\n",
    "outputs = Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "# Build the model\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "# Define early stopping callback\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "# Train the model with early stopping\n",
    "history = model.fit(\n",
    "    x_train, y_train, \n",
    "    epochs=10, \n",
    "    batch_size=64, \n",
    "    validation_data=(x_test, y_test), \n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
    "print('Test Loss:', test_loss)\n",
    "print('Test Accuracy:', test_acc)\n",
    "\n",
    "# Predict on a sample text\n",
    "# Predict on a sample text\n",
    "sample_text = \"The movie was not good. The animation and the graphics were terrible. I would not recommend this movie.\"\n",
    "\n",
    "# Tokenize and preprocess the sample text\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Convert the sample text to integer sequences\n",
    "def preprocess_text(text):\n",
    "    words = text.lower().split()\n",
    "    encoded_text = [word_index.get(word, 2) + 3 for word in words]  # +3 because IMDB indices are offset\n",
    "    padded_text = tf.keras.preprocessing.sequence.pad_sequences([encoded_text], maxlen=max_length)\n",
    "    return padded_text\n",
    "\n",
    "sample_padded = preprocess_text(sample_text)\n",
    "\n",
    "# Predict sentiment\n",
    "predictions = model.predict(sample_padded)\n",
    "print(f'Prediction: {\"Positive\" if predictions[0][0] > 0.5 else \"Negative\"} with score: {predictions[0][0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('simpleRNN_IMDB_sentiment Analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "model = load_model('simpleRNN_IMDB_sentiment Analysis.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 583ms/step\n",
      "Prediction: Negative with score: 0.0660569965839386\n"
     ]
    }
   ],
   "source": [
    "# Predict on a sample text\n",
    "# Predict on a sample text\n",
    "sample_text = \"And then we have Jake with his closet which totally ruins all the film! I expected to see a BOOGEYMAN similar movie, and instead i watched a drama with some meaningless thriller spots\"\n",
    "\n",
    "# Tokenize and preprocess the sample text\n",
    "word_index = imdb.get_word_index()\n",
    "\n",
    "# Convert the sample text to integer sequences\n",
    "def preprocess_text(text):\n",
    "    words = text.lower().split()\n",
    "    encoded_text = [word_index.get(word, 2) + 3 for word in words]  # +3 because IMDB indices are offset\n",
    "    padded_text = tf.keras.preprocessing.sequence.pad_sequences([encoded_text], maxlen=max_length)\n",
    "    return padded_text\n",
    "\n",
    "sample_padded = preprocess_text(sample_text)\n",
    "\n",
    "# Predict sentiment\n",
    "predictions = model.predict(sample_padded)\n",
    "print(f'Prediction: {\"Positive\" if predictions[0][0] > 0.5 else \"Negative\"} with score: {predictions[0][0]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    words = text.lower().split()\n",
    "    encoded_review = [word_index.get(word, 2) + 3 for word in words]\n",
    "    padded_review = sequence.pad_sequences([encoded_review], maxlen=500)\n",
    "    \n",
    "    # Debugging prints\n",
    "    print(f\"Original Text: {text}\")\n",
    "    # print(f\"Encoded Review: {encoded_review}\")\n",
    "    # print(f\"Padded Review Shape: {padded_review.shape}\")\n",
    "    \n",
    "    return padded_review\n",
    "\n",
    "def predict_sentiment(review):\n",
    "    preprocessed_input = preprocess_text(review)\n",
    "    \n",
    "    # Debugging prints\n",
    "    print(f\"Preprocessed Input for Prediction: {preprocessed_input}\")\n",
    "    \n",
    "    prediction = model.predict(preprocessed_input)\n",
    "    \n",
    "    sentiment = 'Positive' if prediction[0][0] > 0.05 else 'Negative'\n",
    "    \n",
    "    # Debugging prints\n",
    "    print(f\"Raw Prediction Score: {prediction[0][0]}\")\n",
    "    \n",
    "    return sentiment, prediction[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_review = \"That movie was fantastic .\"\n",
    "sentiment, score = predict_sentiment(example_review)\n",
    "\n",
    "print(f'Review: {example_review}')\n",
    "print(f'Sentiment: {sentiment}')\n",
    "print(f'Prediction Score: {score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    words = text.lower().split()\n",
    "    encoded_text = [word_index.get(word, 2) + 3 for word in words]  # +3 because IMDB indices are offset\n",
    "    padded_text = tf.keras.preprocessing.sequence.pad_sequences([encoded_text], maxlen=max_length)\n",
    "    return padded_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    words = text.lower().split()\n",
    "    encoded_text = [word_index.get(word, 2) + 3 for word in words]  # +3 because IMDB indices are offset\n",
    "    padded_text = tf.keras.preprocessing.sequence.pad_sequences([encoded_text], maxlen=max_length)\n",
    "    return padded_text\n",
    "\n",
    "# Function to predict sentiment of a review\n",
    "def predict_sentiment(review):\n",
    "    preprocessed_input = preprocess_text(review)\n",
    "    prediction = model.predict(preprocessed_input)\n",
    "    sentiment = 'Positive' if prediction[0][0] > 0.5 else 'Negative'\n",
    "    score = prediction[0][0]\n",
    "    return sentiment, score\n",
    "\n",
    "# Test the function on example reviews\n",
    "example_reviews = [\n",
    "    \"The movie was fantastic, full of suspense and drama!\",\n",
    "    \"I didn't like the movie, the plot was terrible and the acting was bad.\",\n",
    "    \"It was an okay movie, not too bad but not great either.\",\n",
    "    \"Absolutely loved this film, will watch it again.\",\n",
    "    \"Worst movie I've ever seen, do not recommend!\"\n",
    "]\n",
    "\n",
    "for review in example_reviews:\n",
    "    sentiment, score = predict_sentiment(review)\n",
    "    print(f'Review: \"{review}\"\\nPredicted Sentiment: {sentiment}, Score: {score:.4f}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
